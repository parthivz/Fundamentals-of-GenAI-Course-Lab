{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthivz/Fundamentals-of-GenAI-Course-Lab/blob/main/PyTorch_3_Exercise_PyTorch_Dataset_Dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5yuXDh4W5zq"
      },
      "source": [
        "## **TODO:** Download the yelp dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-mOZ6oR8q6K"
      },
      "source": [
        "## Use PyTorch `Dataset` and `Dataloader` with a structured dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3oKWxWlmun66"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch as pt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "pt.set_default_dtype(pt.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p8W9ZSL9I9w"
      },
      "source": [
        "Read the files that match `part-*.csv` from the `data` subdirectory into a Pandas data frame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I3fr0_i_YEFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Define URL and download path\n",
        "URL = \"https://s3.amazonaws.com/courses.axel.net/data_working_with_pytorch.zip\"\n",
        "ZIP_PATH = \"data_working_with_pytorch.zip\"\n",
        "EXTRACT_PATH = \"data\"\n",
        "\n",
        "# Download the dataset if not already downloaded\n",
        "if not os.path.exists(ZIP_PATH):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(URL, ZIP_PATH)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# Extract the dataset if not already extracted\n",
        "if not os.path.exists(EXTRACT_PATH):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EXTRACT_PATH)\n",
        "    print(\"Extraction complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RULBK-A9X7D"
      },
      "source": [
        "## Explore the `df` data frame, including the column names, the first few rows of the dataset, and the data frame's memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3vDc_ZNI9ilK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cc1ef8-1d32-4148-9f77-9577258f6e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 6368133 rows and 7 columns.\n",
            "Index(['fareamount', 'origindatetime_tr', 'origin_block_latitude',\n",
            "       'origin_block_longitude', 'destination_block_latitude',\n",
            "       'destination_block_longitude', 'id'],\n",
            "      dtype='object')\n",
            "   fareamount origindatetime_tr  origin_block_latitude  \\\n",
            "0        4.87  06/01/2017 07:00              38.898314   \n",
            "1       12.70  06/01/2017 14:00              38.904683   \n",
            "2        5.14  06/01/2017 12:00              38.910635   \n",
            "3        5.14  06/02/2017 13:00              38.889184   \n",
            "4       14.32  06/01/2017 13:00              38.901336   \n",
            "\n",
            "   origin_block_longitude  destination_block_latitude  \\\n",
            "0              -77.028849                   38.902521   \n",
            "1              -77.046645                   38.940181   \n",
            "2              -77.042514                   38.909652   \n",
            "3              -77.021907                   38.897207   \n",
            "4              -77.037534                   38.942216   \n",
            "\n",
            "   destination_block_longitude  \\\n",
            "0                   -77.030791   \n",
            "1                   -77.061193   \n",
            "2                   -77.033254   \n",
            "3                   -77.023477   \n",
            "4                   -77.073508   \n",
            "\n",
            "                                                  id  \n",
            "0  751d10ef2403c770a3bd4e220db8594b656d6774962b63...  \n",
            "1  a9ddc1ab38a3cc3f360e4d2408678d707658762c418e6c...  \n",
            "2  1f804117b3d98193b5ab7fddc15a543a8165cd60b6b20e...  \n",
            "3  21af1912855db837c7892fb073f4c59678c305aec0b23b...  \n",
            "4  26dcdd256e6269e4c6f1ccd2119c345c4deed788a35082...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6368133 entries, 0 to 3289205\n",
            "Data columns (total 7 columns):\n",
            " #   Column                       Dtype  \n",
            "---  ------                       -----  \n",
            " 0   fareamount                   float64\n",
            " 1   origindatetime_tr            object \n",
            " 2   origin_block_latitude        float64\n",
            " 3   origin_block_longitude       float64\n",
            " 4   destination_block_latitude   float64\n",
            " 5   destination_block_longitude  float64\n",
            " 6   id                           object \n",
            "dtypes: float64(5), object(2)\n",
            "memory usage: 1.3 GB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch as pt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "pt.set_default_dtype(pt.float64)  # Ensure consistency in precision\n",
        "\n",
        "# Read all CSV files matching \"part-*.csv\" into a single DataFrame\n",
        "df = pd.concat([pd.read_csv(os.path.join(EXTRACT_PATH, file))\n",
        "                for file in os.listdir(EXTRACT_PATH) if file.startswith(\"part-\")])\n",
        "\n",
        "print(f\"Dataset loaded with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "# Display column names\n",
        "print(df.columns)\n",
        "\n",
        "# Show the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Check memory usage\n",
        "print(df.info(memory_usage=\"deep\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-kVFhml9p0x"
      },
      "source": [
        "## Drop the `origindatetime_tr` column from the data frame.\n",
        "\n",
        "For now you are going to predict the taxi fare just based on the lat/lon coordinates of the pickup and the drop off locations. Remove the `origindatetime_tr` column from the data frame in your working dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=[\"origindatetime_tr\"])  # Drop timestamp column\n"
      ],
      "metadata": {
        "id": "QKJpQ-dayCS5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aA0NkUA_x1M"
      },
      "source": [
        "## Sample 10% of your working dataset into a test dataset data frame\n",
        "\n",
        "* **hint:** use the Pandas `sample` function with the dataframe. Specify a value for the `random_state` to achieve reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nsh_vPXiZr9J"
      },
      "outputs": [],
      "source": [
        "# Sample 10% for testing\n",
        "test_df = df.sample(frac=0.1, random_state=42)  # Reproducible results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FschugACN-"
      },
      "source": [
        "## Drop the rows that exist in your test dataset from the working dataset to produce a training dataset.\n",
        "\n",
        "* **hint** DataFrame's `drop` function can use index values from a data frame to drop specific rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CT-b2IlIZ9FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2f4134-b63a-49c9-9a54-e9b239cc9559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 5177451, Test size: 636813\n"
          ]
        }
      ],
      "source": [
        "# Drop test rows from training set\n",
        "train_df = df.drop(test_df.index)\n",
        "\n",
        "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R0P1sBeAX15"
      },
      "source": [
        "## Define 2 Python lists: 1st for the feature column names; 2nd for the target column name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s62k_A-Ga-0x"
      },
      "outputs": [],
      "source": [
        "# Define feature columns (latitude, longitude) and target column (fare)\n",
        "feature_columns = [\"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\"]\n",
        "target_column = [\"fareamount\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttQDA-m8AgQx"
      },
      "source": [
        "## Create `X` and `y` tensors with the values of your feature and target columns in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hX2dlZgpbA6I"
      },
      "outputs": [],
      "source": [
        "# Ensure all feature columns exist\n",
        "available_features = [col for col in feature_columns if col in train_df.columns]\n",
        "\n",
        "# Convert train data to tensors\n",
        "X_train = pt.tensor(train_df[available_features].values, dtype=pt.float64)\n",
        "y_train = pt.tensor(train_df[target_column].values, dtype=pt.float64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQm_SJFDAqqn"
      },
      "source": [
        "## Create a `TensorDataset` instance with the `y` and `X` tensors (in that order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ffqTuheNbLpj"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElrnaKEtAyEg"
      },
      "source": [
        "## Create a `DataLoader` instance specifying a custom batch size\n",
        "\n",
        "A batch size of `2 ** 18 = 262,144` should work well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-1YyY4tgbalF"
      },
      "outputs": [],
      "source": [
        "# Define batch size\n",
        "BATCH_SIZE = 2 ** 18  # 262,144\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA-3bXKABCW_"
      },
      "source": [
        "## Create a model using `nn.Linear`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6vYGtKDeajQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1a89cc-899d-4f99-ff68-51d00ca7d97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        }
      ],
      "source": [
        "# Define a simple linear regression model\n",
        "class TaxiFareModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(len(feature_columns), 1)  # Input features → 1 output (fare)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate model\n",
        "model = TaxiFareModel()\n",
        "model = nn.Linear(X_train.shape[1], 1).double()  # Convert model to float64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UvsR9gBGXj"
      },
      "source": [
        "## Create an instance of the `AdamW` optimizer for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vPFF7EtFBKes"
      },
      "outputs": [],
      "source": [
        "optimizer = pt.optim.AdamW(model.parameters(), lr=0.01)  # AdamW for stability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz7LW-TnBNJu"
      },
      "source": [
        "## Declare your `forward`, `loss` and `metric` functions\n",
        "\n",
        "* **hint:** if you are tried of computing MSE by hand you can use `nn.functional.mse_loss` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0T3aWJEZdiVH"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "def loss_fn(y_pred, y_true):\n",
        "    return F.mse_loss(y_pred, y_true)\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "def rmse(y_pred, y_true):\n",
        "    return pt.sqrt(loss_fn(y_pred, y_true))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfu2ejeoBfpQ"
      },
      "source": [
        "## Iterate over the batches returned by your `DataLoader` instance\n",
        "\n",
        "For every step of gradient descent, print out the MSE, RMSE, and the batch index\n",
        "* **hint:** you can use Python's `enumerable` for an iterable\n",
        "* **hint:** the batch returned by the `enumerable` has the same contents as your `TensorDataset` instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bVjF8VYwbATZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9c8794-6d9a-4c8c-a1de-cacb6ccfcf4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: MSE = 115.8499, RMSE = 10.7634\n",
            "Batch 1: MSE = 115.3742, RMSE = 10.7412\n",
            "Batch 2: MSE = 114.7101, RMSE = 10.7103\n",
            "Batch 3: MSE = 114.1856, RMSE = 10.6858\n",
            "Batch 4: MSE = 114.3140, RMSE = 10.6918\n",
            "Batch 5: MSE = 114.8712, RMSE = 10.7178\n",
            "Batch 6: MSE = 114.0713, RMSE = 10.6804\n",
            "Batch 7: MSE = 113.5932, RMSE = 10.6580\n",
            "Batch 8: MSE = 113.2199, RMSE = 10.6405\n",
            "Batch 9: MSE = 113.0383, RMSE = 10.6319\n",
            "Batch 10: MSE = 113.5653, RMSE = 10.6567\n",
            "Batch 11: MSE = 113.2965, RMSE = 10.6441\n",
            "Batch 12: MSE = 113.1279, RMSE = 10.6362\n",
            "Batch 13: MSE = 112.7565, RMSE = 10.6187\n",
            "Batch 14: MSE = 112.4849, RMSE = 10.6059\n",
            "Batch 15: MSE = 112.6724, RMSE = 10.6147\n",
            "Batch 16: MSE = 112.4410, RMSE = 10.6038\n",
            "Batch 17: MSE = 112.2282, RMSE = 10.5938\n",
            "Batch 18: MSE = 111.9377, RMSE = 10.5801\n",
            "Batch 19: MSE = 111.4893, RMSE = 10.5589\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "    # Forward pass\n",
        "    y_pred = model(X_batch)\n",
        "\n",
        "    # Compute MSE loss\n",
        "    loss = loss_fn(y_pred, y_batch.view(-1, 1))  # Ensure y_batch has correct shape\n",
        "\n",
        "    # Compute RMSE\n",
        "    rmse_value = pt.sqrt(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Batch {batch_idx}: MSE = {loss.item():.4f}, RMSE = {rmse_value.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVl44Jq5CApl"
      },
      "source": [
        "## Implement 10 epochs of gradient descent training\n",
        "\n",
        "For every step of gradient descent, printout the MSE, RMSE, epoch index, and batch index.\n",
        "\n",
        "* **hint:** you can call `enumerate(DataLoader)` repeatedly in a `for` loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hHtI3TB8ewaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84d412d-ce69-488e-e1da-94e26b8d8d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 0: MSE = 111.5720, RMSE = 10.5628\n",
            "Epoch 1, Batch 1: MSE = 111.0290, RMSE = 10.5370\n",
            "Epoch 1, Batch 2: MSE = 110.8370, RMSE = 10.5279\n",
            "Epoch 1, Batch 3: MSE = 110.2292, RMSE = 10.4990\n",
            "Epoch 1, Batch 4: MSE = 110.6765, RMSE = 10.5203\n",
            "Epoch 1, Batch 5: MSE = 110.0693, RMSE = 10.4914\n",
            "Epoch 1, Batch 6: MSE = 110.1186, RMSE = 10.4937\n",
            "Epoch 1, Batch 7: MSE = 109.5369, RMSE = 10.4660\n",
            "Epoch 1, Batch 8: MSE = 110.9675, RMSE = 10.5341\n",
            "Epoch 1, Batch 9: MSE = 110.0242, RMSE = 10.4892\n",
            "Epoch 1, Batch 10: MSE = 109.6585, RMSE = 10.4718\n",
            "Epoch 1, Batch 11: MSE = 109.7333, RMSE = 10.4754\n",
            "Epoch 1, Batch 12: MSE = 109.4427, RMSE = 10.4615\n",
            "Epoch 1, Batch 13: MSE = 108.9281, RMSE = 10.4369\n",
            "Epoch 1, Batch 14: MSE = 108.4539, RMSE = 10.4141\n",
            "Epoch 1, Batch 15: MSE = 109.0394, RMSE = 10.4422\n",
            "Epoch 1, Batch 16: MSE = 108.5257, RMSE = 10.4176\n",
            "Epoch 1, Batch 17: MSE = 109.0430, RMSE = 10.4424\n",
            "Epoch 1, Batch 18: MSE = 107.9345, RMSE = 10.3892\n",
            "Epoch 1, Batch 19: MSE = 107.7562, RMSE = 10.3806\n",
            "Epoch 2, Batch 0: MSE = 108.1901, RMSE = 10.4014\n",
            "Epoch 2, Batch 1: MSE = 108.0295, RMSE = 10.3937\n",
            "Epoch 2, Batch 2: MSE = 107.2595, RMSE = 10.3566\n",
            "Epoch 2, Batch 3: MSE = 106.4159, RMSE = 10.3158\n",
            "Epoch 2, Batch 4: MSE = 106.2466, RMSE = 10.3076\n",
            "Epoch 2, Batch 5: MSE = 106.4859, RMSE = 10.3192\n",
            "Epoch 2, Batch 6: MSE = 106.6418, RMSE = 10.3268\n",
            "Epoch 2, Batch 7: MSE = 106.2498, RMSE = 10.3078\n",
            "Epoch 2, Batch 8: MSE = 106.1470, RMSE = 10.3028\n",
            "Epoch 2, Batch 9: MSE = 105.9876, RMSE = 10.2950\n",
            "Epoch 2, Batch 10: MSE = 105.5851, RMSE = 10.2755\n",
            "Epoch 2, Batch 11: MSE = 105.2196, RMSE = 10.2577\n",
            "Epoch 2, Batch 12: MSE = 105.3810, RMSE = 10.2655\n",
            "Epoch 2, Batch 13: MSE = 105.8413, RMSE = 10.2879\n",
            "Epoch 2, Batch 14: MSE = 105.9731, RMSE = 10.2943\n",
            "Epoch 2, Batch 15: MSE = 104.7315, RMSE = 10.2338\n",
            "Epoch 2, Batch 16: MSE = 105.0700, RMSE = 10.2504\n",
            "Epoch 2, Batch 17: MSE = 105.3557, RMSE = 10.2643\n",
            "Epoch 2, Batch 18: MSE = 104.6923, RMSE = 10.2319\n",
            "Epoch 2, Batch 19: MSE = 104.9207, RMSE = 10.2431\n",
            "Epoch 3, Batch 0: MSE = 104.7803, RMSE = 10.2362\n",
            "Epoch 3, Batch 1: MSE = 103.7792, RMSE = 10.1872\n",
            "Epoch 3, Batch 2: MSE = 103.9017, RMSE = 10.1932\n",
            "Epoch 3, Batch 3: MSE = 103.4831, RMSE = 10.1727\n",
            "Epoch 3, Batch 4: MSE = 103.8027, RMSE = 10.1884\n",
            "Epoch 3, Batch 5: MSE = 103.1572, RMSE = 10.1566\n",
            "Epoch 3, Batch 6: MSE = 103.2426, RMSE = 10.1608\n",
            "Epoch 3, Batch 7: MSE = 103.3138, RMSE = 10.1643\n",
            "Epoch 3, Batch 8: MSE = 103.0029, RMSE = 10.1490\n",
            "Epoch 3, Batch 9: MSE = 102.8093, RMSE = 10.1395\n",
            "Epoch 3, Batch 10: MSE = 102.2167, RMSE = 10.1102\n",
            "Epoch 3, Batch 11: MSE = 101.9039, RMSE = 10.0947\n",
            "Epoch 3, Batch 12: MSE = 101.6487, RMSE = 10.0821\n",
            "Epoch 3, Batch 13: MSE = 101.4758, RMSE = 10.0735\n",
            "Epoch 3, Batch 14: MSE = 102.0864, RMSE = 10.1038\n",
            "Epoch 3, Batch 15: MSE = 100.6002, RMSE = 10.0300\n",
            "Epoch 3, Batch 16: MSE = 101.3885, RMSE = 10.0692\n",
            "Epoch 3, Batch 17: MSE = 100.8937, RMSE = 10.0446\n",
            "Epoch 3, Batch 18: MSE = 101.2533, RMSE = 10.0625\n",
            "Epoch 3, Batch 19: MSE = 100.5265, RMSE = 10.0263\n",
            "Epoch 4, Batch 0: MSE = 100.5866, RMSE = 10.0293\n",
            "Epoch 4, Batch 1: MSE = 100.5726, RMSE = 10.0286\n",
            "Epoch 4, Batch 2: MSE = 100.0647, RMSE = 10.0032\n",
            "Epoch 4, Batch 3: MSE = 100.4131, RMSE = 10.0206\n",
            "Epoch 4, Batch 4: MSE = 100.1068, RMSE = 10.0053\n",
            "Epoch 4, Batch 5: MSE = 99.2261, RMSE = 9.9612\n",
            "Epoch 4, Batch 6: MSE = 99.6977, RMSE = 9.9849\n",
            "Epoch 4, Batch 7: MSE = 99.9574, RMSE = 9.9979\n",
            "Epoch 4, Batch 8: MSE = 100.0688, RMSE = 10.0034\n",
            "Epoch 4, Batch 9: MSE = 98.9313, RMSE = 9.9464\n",
            "Epoch 4, Batch 10: MSE = 98.1516, RMSE = 9.9072\n",
            "Epoch 4, Batch 11: MSE = 98.8562, RMSE = 9.9426\n",
            "Epoch 4, Batch 12: MSE = 98.8106, RMSE = 9.9404\n",
            "Epoch 4, Batch 13: MSE = 98.8387, RMSE = 9.9418\n",
            "Epoch 4, Batch 14: MSE = 98.2595, RMSE = 9.9126\n",
            "Epoch 4, Batch 15: MSE = 97.7981, RMSE = 9.8893\n",
            "Epoch 4, Batch 16: MSE = 98.1421, RMSE = 9.9067\n",
            "Epoch 4, Batch 17: MSE = 96.9906, RMSE = 9.8484\n",
            "Epoch 4, Batch 18: MSE = 97.6343, RMSE = 9.8810\n",
            "Epoch 4, Batch 19: MSE = 97.7443, RMSE = 9.8866\n",
            "Epoch 5, Batch 0: MSE = 97.2660, RMSE = 9.8624\n",
            "Epoch 5, Batch 1: MSE = 97.1839, RMSE = 9.8582\n",
            "Epoch 5, Batch 2: MSE = 97.1057, RMSE = 9.8542\n",
            "Epoch 5, Batch 3: MSE = 96.9691, RMSE = 9.8473\n",
            "Epoch 5, Batch 4: MSE = 96.5418, RMSE = 9.8256\n",
            "Epoch 5, Batch 5: MSE = 96.8657, RMSE = 9.8420\n",
            "Epoch 5, Batch 6: MSE = 96.7226, RMSE = 9.8348\n",
            "Epoch 5, Batch 7: MSE = 95.6631, RMSE = 9.7807\n",
            "Epoch 5, Batch 8: MSE = 95.8857, RMSE = 9.7921\n",
            "Epoch 5, Batch 9: MSE = 95.3513, RMSE = 9.7648\n",
            "Epoch 5, Batch 10: MSE = 95.9492, RMSE = 9.7954\n",
            "Epoch 5, Batch 11: MSE = 95.9225, RMSE = 9.7940\n",
            "Epoch 5, Batch 12: MSE = 95.3081, RMSE = 9.7626\n",
            "Epoch 5, Batch 13: MSE = 94.4252, RMSE = 9.7173\n",
            "Epoch 5, Batch 14: MSE = 95.1236, RMSE = 9.7531\n",
            "Epoch 5, Batch 15: MSE = 95.0365, RMSE = 9.7487\n",
            "Epoch 5, Batch 16: MSE = 94.6311, RMSE = 9.7279\n",
            "Epoch 5, Batch 17: MSE = 94.2657, RMSE = 9.7091\n",
            "Epoch 5, Batch 18: MSE = 94.3472, RMSE = 9.7132\n",
            "Epoch 5, Batch 19: MSE = 93.9195, RMSE = 9.6912\n",
            "Epoch 6, Batch 0: MSE = 94.0129, RMSE = 9.6960\n",
            "Epoch 6, Batch 1: MSE = 93.7878, RMSE = 9.6844\n",
            "Epoch 6, Batch 2: MSE = 93.4082, RMSE = 9.6648\n",
            "Epoch 6, Batch 3: MSE = 93.5049, RMSE = 9.6698\n",
            "Epoch 6, Batch 4: MSE = 93.6647, RMSE = 9.6781\n",
            "Epoch 6, Batch 5: MSE = 92.8732, RMSE = 9.6371\n",
            "Epoch 6, Batch 6: MSE = 93.1018, RMSE = 9.6489\n",
            "Epoch 6, Batch 7: MSE = 92.3787, RMSE = 9.6114\n",
            "Epoch 6, Batch 8: MSE = 92.3264, RMSE = 9.6087\n",
            "Epoch 6, Batch 9: MSE = 92.6829, RMSE = 9.6272\n",
            "Epoch 6, Batch 10: MSE = 92.6545, RMSE = 9.6257\n",
            "Epoch 6, Batch 11: MSE = 92.2165, RMSE = 9.6029\n",
            "Epoch 6, Batch 12: MSE = 92.1760, RMSE = 9.6008\n",
            "Epoch 6, Batch 13: MSE = 92.9223, RMSE = 9.6396\n",
            "Epoch 6, Batch 14: MSE = 91.7315, RMSE = 9.5777\n",
            "Epoch 6, Batch 15: MSE = 91.5774, RMSE = 9.5696\n",
            "Epoch 6, Batch 16: MSE = 91.2899, RMSE = 9.5546\n",
            "Epoch 6, Batch 17: MSE = 91.5526, RMSE = 9.5683\n",
            "Epoch 6, Batch 18: MSE = 91.7559, RMSE = 9.5789\n",
            "Epoch 6, Batch 19: MSE = 90.9557, RMSE = 9.5371\n",
            "Epoch 7, Batch 0: MSE = 91.1575, RMSE = 9.5476\n",
            "Epoch 7, Batch 1: MSE = 90.4435, RMSE = 9.5102\n",
            "Epoch 7, Batch 2: MSE = 90.2313, RMSE = 9.4990\n",
            "Epoch 7, Batch 3: MSE = 90.1021, RMSE = 9.4922\n",
            "Epoch 7, Batch 4: MSE = 90.3906, RMSE = 9.5074\n",
            "Epoch 7, Batch 5: MSE = 89.7875, RMSE = 9.4756\n",
            "Epoch 7, Batch 6: MSE = 90.5007, RMSE = 9.5132\n",
            "Epoch 7, Batch 7: MSE = 89.5957, RMSE = 9.4655\n",
            "Epoch 7, Batch 8: MSE = 90.6763, RMSE = 9.5224\n",
            "Epoch 7, Batch 9: MSE = 89.9517, RMSE = 9.4843\n",
            "Epoch 7, Batch 10: MSE = 89.3328, RMSE = 9.4516\n",
            "Epoch 7, Batch 11: MSE = 89.2778, RMSE = 9.4487\n",
            "Epoch 7, Batch 12: MSE = 88.8338, RMSE = 9.4252\n",
            "Epoch 7, Batch 13: MSE = 88.8990, RMSE = 9.4286\n",
            "Epoch 7, Batch 14: MSE = 89.2058, RMSE = 9.4449\n",
            "Epoch 7, Batch 15: MSE = 89.1810, RMSE = 9.4436\n",
            "Epoch 7, Batch 16: MSE = 88.1032, RMSE = 9.3863\n",
            "Epoch 7, Batch 17: MSE = 88.0311, RMSE = 9.3825\n",
            "Epoch 7, Batch 18: MSE = 87.9951, RMSE = 9.3806\n",
            "Epoch 7, Batch 19: MSE = 86.8899, RMSE = 9.3215\n",
            "Epoch 8, Batch 0: MSE = 88.3431, RMSE = 9.3991\n",
            "Epoch 8, Batch 1: MSE = 87.5570, RMSE = 9.3572\n",
            "Epoch 8, Batch 2: MSE = 87.0781, RMSE = 9.3316\n",
            "Epoch 8, Batch 3: MSE = 87.9417, RMSE = 9.3777\n",
            "Epoch 8, Batch 4: MSE = 87.1016, RMSE = 9.3328\n",
            "Epoch 8, Batch 5: MSE = 87.9479, RMSE = 9.3781\n",
            "Epoch 8, Batch 6: MSE = 86.5369, RMSE = 9.3025\n",
            "Epoch 8, Batch 7: MSE = 86.6950, RMSE = 9.3110\n",
            "Epoch 8, Batch 8: MSE = 86.5759, RMSE = 9.3046\n",
            "Epoch 8, Batch 9: MSE = 86.4679, RMSE = 9.2988\n",
            "Epoch 8, Batch 10: MSE = 85.6631, RMSE = 9.2554\n",
            "Epoch 8, Batch 11: MSE = 86.6888, RMSE = 9.3107\n",
            "Epoch 8, Batch 12: MSE = 85.3605, RMSE = 9.2391\n",
            "Epoch 8, Batch 13: MSE = 86.6323, RMSE = 9.3076\n",
            "Epoch 8, Batch 14: MSE = 85.3454, RMSE = 9.2383\n",
            "Epoch 8, Batch 15: MSE = 85.3039, RMSE = 9.2360\n",
            "Epoch 8, Batch 16: MSE = 85.6310, RMSE = 9.2537\n",
            "Epoch 8, Batch 17: MSE = 85.1540, RMSE = 9.2279\n",
            "Epoch 8, Batch 18: MSE = 85.7559, RMSE = 9.2604\n",
            "Epoch 8, Batch 19: MSE = 85.6565, RMSE = 9.2551\n",
            "Epoch 9, Batch 0: MSE = 84.8689, RMSE = 9.2124\n",
            "Epoch 9, Batch 1: MSE = 85.2652, RMSE = 9.2339\n",
            "Epoch 9, Batch 2: MSE = 84.6592, RMSE = 9.2010\n",
            "Epoch 9, Batch 3: MSE = 84.1628, RMSE = 9.1740\n",
            "Epoch 9, Batch 4: MSE = 84.8085, RMSE = 9.2092\n",
            "Epoch 9, Batch 5: MSE = 84.0233, RMSE = 9.1664\n",
            "Epoch 9, Batch 6: MSE = 84.2607, RMSE = 9.1794\n",
            "Epoch 9, Batch 7: MSE = 84.3377, RMSE = 9.1836\n",
            "Epoch 9, Batch 8: MSE = 84.1164, RMSE = 9.1715\n",
            "Epoch 9, Batch 9: MSE = 83.2783, RMSE = 9.1257\n",
            "Epoch 9, Batch 10: MSE = 83.9392, RMSE = 9.1618\n",
            "Epoch 9, Batch 11: MSE = 83.0322, RMSE = 9.1122\n",
            "Epoch 9, Batch 12: MSE = 83.0771, RMSE = 9.1147\n",
            "Epoch 9, Batch 13: MSE = 83.6173, RMSE = 9.1442\n",
            "Epoch 9, Batch 14: MSE = 82.5800, RMSE = 9.0874\n",
            "Epoch 9, Batch 15: MSE = 82.8018, RMSE = 9.0995\n",
            "Epoch 9, Batch 16: MSE = 82.6011, RMSE = 9.0885\n",
            "Epoch 9, Batch 17: MSE = 81.8417, RMSE = 9.0466\n",
            "Epoch 9, Batch 18: MSE = 82.3919, RMSE = 9.0770\n",
            "Epoch 9, Batch 19: MSE = 82.1387, RMSE = 9.0630\n",
            "Epoch 10, Batch 0: MSE = 82.4656, RMSE = 9.0811\n",
            "Epoch 10, Batch 1: MSE = 82.1976, RMSE = 9.0663\n",
            "Epoch 10, Batch 2: MSE = 81.7254, RMSE = 9.0402\n",
            "Epoch 10, Batch 3: MSE = 81.4658, RMSE = 9.0258\n",
            "Epoch 10, Batch 4: MSE = 81.1263, RMSE = 9.0070\n",
            "Epoch 10, Batch 5: MSE = 81.7498, RMSE = 9.0416\n",
            "Epoch 10, Batch 6: MSE = 81.3600, RMSE = 9.0200\n",
            "Epoch 10, Batch 7: MSE = 81.4222, RMSE = 9.0234\n",
            "Epoch 10, Batch 8: MSE = 81.8151, RMSE = 9.0452\n",
            "Epoch 10, Batch 9: MSE = 80.4134, RMSE = 8.9673\n",
            "Epoch 10, Batch 10: MSE = 80.8275, RMSE = 8.9904\n",
            "Epoch 10, Batch 11: MSE = 80.7227, RMSE = 8.9846\n",
            "Epoch 10, Batch 12: MSE = 80.5916, RMSE = 8.9773\n",
            "Epoch 10, Batch 13: MSE = 80.5509, RMSE = 8.9750\n",
            "Epoch 10, Batch 14: MSE = 80.1264, RMSE = 8.9513\n",
            "Epoch 10, Batch 15: MSE = 79.6876, RMSE = 8.9268\n",
            "Epoch 10, Batch 16: MSE = 79.3368, RMSE = 8.9071\n",
            "Epoch 10, Batch 17: MSE = 79.5663, RMSE = 8.9200\n",
            "Epoch 10, Batch 18: MSE = 79.9465, RMSE = 8.9413\n",
            "Epoch 10, Batch 19: MSE = 79.2501, RMSE = 8.9023\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10  # Number of epochs\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        y_pred = model(X_batch)\n",
        "\n",
        "        # Compute MSE loss\n",
        "        loss = loss_fn(y_pred, y_batch.view(-1, 1))\n",
        "\n",
        "        # Compute RMSE\n",
        "        rmse_value = pt.sqrt(loss)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Epoch {epoch + 1}, Batch {batch_idx}: MSE = {loss.item():.4f}, RMSE = {rmse_value.item():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}