{"cells":[{"cell_type":"markdown","metadata":{"id":"vFnCDm9kpdW4"},"source":["## Classify the concentric circles dataset using a not-so-deep neural network with non-linear activations"]},{"cell_type":"markdown","metadata":{"id":"dj5FZd7kpo6s"},"source":["The following has your usual imports along with the data generation `circle` and data visualization `decision_boundary` functions predefined for you. You'll start by learning about `circle` and progress to learning about `decision_boundary` later, towards the end of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xc1d4ZuFCmgW"},"outputs":[],"source":["import torch as pt\n","from torch import nn\n","import matplotlib.pyplot as plt\n","import math\n","%matplotlib inline\n","pt.manual_seed(1337);\n","\n","def circle(points = 100, radius_lower = 9., radius_upper = 10.):\n","  radius = pt.zeros(points).uniform_(radius_lower, radius_upper)\n","  angle = pt.zeros(points).uniform_(0, 2 * math.pi)\n","\n","  x = radius * pt.sin(angle)\n","  y = radius * pt.cos(angle)\n","\n","  return x,y\n","\n","def decision_boundary(forward_fn, x_min = -15, x_max = 15, y_min = -15., y_max = 15., step = 0.25):\n","  import numpy as np\n","  xx, yy = np.meshgrid(np.arange(x_min, x_max, step),\n","                      np.arange(y_min, y_max, step))\n","\n","  Xg = np.hstack( (xx.ravel()[:, None], yy.ravel()[:, None]) )\n","\n","\n","  X_test = pt.from_numpy(Xg).to(pt.float)\n","  y_test = forward_fn(X_test).argmax(dim = 1).detach().numpy()\n","\n","  Z = -1 * y_test.reshape(xx.shape)\n","\n","  fig = plt.figure()\n","  plt.axis(\"equal\")\n","  plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.4);\n","  plt.scatter(X[:NUM, 0], X[:NUM, 1], color = 'orange');\n","  plt.scatter(X[NUM:, 0], X[NUM:, 1])"]},{"cell_type":"markdown","metadata":{"id":"5JI-0dRHqY-s"},"source":["## Generate a tuple of tensors for the `x` and `y` coordinates of the inner circle consisting of `100` data points by using the `circle` function.\n","\n","For example, if you use\n","\n","<code>\n","  circle(100, radius_lower = 6., radius_upper=9.)\n","</code>\n","\n","The function is going to return a tuple with two tensors, each with a shape of `torch.Size([100])`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6z8oIC9qCEZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"deqKKSxFrEaa"},"source":["## Figure out how to concatenate the `x` and `y` coordinates from the `circle` method into a single tensor\n","\n","The concatenated tensor should have a of shape `torch.Size([100, 2])`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_u3Q57maq_fw"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FJOA1M4lrTnq"},"source":["## Do the same for the other circle\n","\n","For example, you can use\n","\n","<code>\n","    circle(100, radius_lower = 13., radius_upper=14.)\n","</code>\n","\n","to generate an outer circle.\n","\n","Don't forget to create a `torch.Size([100, 2])` tensor for the outer circle."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_vSClqirvDF"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QTk0JDEmr5yx"},"source":["## Package the values for the circles into a single `X` tensor to use for training.\n","\n","Assuming that you used 100 data points per circle, you should end up with an `X` tensor shaped `torch.Size([200, 2])`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQqi-88Sr6Hz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_76vVNm9sKFx"},"source":["Plot the circles using `scatter`.\n","\n","* **hint:** recall that you can use the `axis(\"equal\")` method to make the x and y axes equal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXO_V4Bidn16"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"npXFl9QPtNVX"},"source":["## Generate the `y` tensor with the classes for each of the circles.\n","\n","PyTorch loss function does not use one-hot encoding and instead relies on ordinal encoding, where each integer represents a different class. For example, one of your circles would be `0`, another circle `1`, next `2`, and so one.\n","\n","In this example you only have 2 circles, so use `0` for one of the circles and `1` for another. Your `y` tensor should have a shape of `torch.Size([200])`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGbYkWovfo8s"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BtoxNL3-t30z"},"source":["## Create a neural net using `nn.Sequential`.\n","\n","It should have a single hidden layer with 3 neurons. All `nn.Linear` layers should use `nn.ReLU` activation function.The output layer should have `nn.LogSoftmax` after the activation function.\n","\n","Don't forget that softmax assumes one-hot encoding for the outputs!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6M-TZcgAv8Zl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SAmB0Fq2wAB7"},"source":["## Implement the `forward` function.\n","\n","If your `nn.Sequential` is named `model`, then the forward pass can be as simple as returning `model(X)`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjcW4Ww6OoFR"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lIpAkQm7wLN1"},"source":["## Complete the implementation of the cross entropy loss\n","\n","If you used `nn.LogSoftmax` as the output layer of your model, you can use the negative log likelihood loss on the output of the softmax layer from `nn.functional.nll_loss`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8m87DmnKwgB_"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3sLVBemiwpsl"},"source":["## Implement a `metric` method that computes the prediction accuracy\n","\n","The accuracy should be on a scale from `0.0` to `1.0`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NanRJR3ZxGlH"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"r2EH_XMsxJ7Y"},"source":["## Create an instance of the `optim.AdamW` optimizer\n","\n","* **hint:** if you used `model` as the name of your instance of `nn.Sequential`, then `model.parameters()` can be used to create the optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joFAHFZJxHrb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lHG0JPNXyiNi"},"source":["## Use the optimizer to discover the neural net weights for classification\n","\n","You may need to perform thousands of steps of gradient descent. Don't forget to properly perform the forward and backward passes. If your instance of `AdamW` is named `optimizer`, then you can perform a gradient descent step using `optimizer.step()`. To zero out the gradients in the model, you can use `optimizer.zero_grad()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36YBZELoYITn"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"A92cBIwjzFx9"},"source":["The `decision_boundary` function declared previously in this notebook uses a grid of test data points to draw the decision boundary for your model. The grid is like a grid on graph paper, so that for every point on the grid, you are going to use the `forward` method of your model to find out the predicted values for the points and color the grid accordingly.\n","\n","The `no_grad` function disables gradient calculations when performing the `forward`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVX4sukThQcg"},"outputs":[],"source":["with pt.no_grad():\n","  decision_boundary(forward)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}