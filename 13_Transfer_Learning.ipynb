{"cells":[{"cell_type":"markdown","source":["HuggingFace is a company with a heavy open source philosophy that makes transformers readily available so you don't have to do what we did before for every application."],"metadata":{"collapsed":false,"id":"EN8OBawJH2xj"},"id":"EN8OBawJH2xj"},{"cell_type":"markdown","source":["## Prep\n"],"metadata":{"collapsed":false,"id":"RK49iFD_H2xm"},"id":"RK49iFD_H2xm"},{"cell_type":"code","execution_count":null,"id":"b1680ad6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1680ad6","outputId":"a0dcf2c2-bd5f-4580-cf08-07c2c9102df2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting transformers\n","  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers) (3.20.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: sentencepiece, dill, responses, multiprocess, transformers, datasets, evaluate\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n"]}],"source":["!pip install -U datasets evaluate transformers transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"id":"56d5b0a4","metadata":{"id":"56d5b0a4"},"outputs":[],"source":["\n","import torch\n","import numpy as np\n","import random\n","import os\n","\n","def set_seeds(seed=42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","set_seeds()\n"]},{"cell_type":"code","execution_count":null,"id":"d2f685b2","metadata":{"id":"d2f685b2"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import numpy as np\n","\n","raw_datasets = None  # load imdb dataset\n","raw_datasets"]},{"cell_type":"code","execution_count":null,"id":"fcd91311","metadata":{"id":"fcd91311"},"outputs":[],"source":["raw_datasets['train'][0]  # Let's see the first review"]},{"cell_type":"code","execution_count":null,"id":"f796e669","metadata":{"id":"f796e669"},"outputs":[],"source":["raw_datasets['train'].features"]},{"cell_type":"code","execution_count":null,"id":"1c5455b5","metadata":{"id":"1c5455b5"},"outputs":[],"source":["\n","from transformers import AutoModelForSequenceClassification\n","\n","model_name = \"google/bert_uncased_L-2_H-128_A-2\"  # Example model\n","model = None # Load the model\n"]},{"cell_type":"code","execution_count":null,"id":"ec4ce600","metadata":{"id":"ec4ce600"},"outputs":[],"source":["\n","from torch.utils.data import DataLoader\n","from transformers import DataCollatorWithPadding\n","from torch.optim import AdamW\n","\n","# Ensure correct tokenization\n","tokenizer = None # Load the tokenizer\n","\n","def tokenize_function(examples):\n","    return None # Tokenize each example\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","# Data collation\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# DataLoader setup\n","train_dataloader = None # Create a dataloader out of the train dataset, suffle it, batch it for 32 examples and use the data collator.\n","eval_dataloader = None # Same\n"]},{"cell_type":"code","source":["# Inspect DataLoader output\n","for batch in train_dataloader:\n","    print(\"Batch shapes:\")\n","    for key, value in batch.items():\n","        if hasattr(value, 'shape'):\n","            print(f\"{key}: {value.shape}\")\n","        else:\n","            print(f\"{key}: {type(value)} - shape attribute not found\")\n","    break  # Remove this break statement to inspect more batches\n"],"metadata":{"id":"lEUQcDFg1WJT"},"id":"lEUQcDFg1WJT","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e0dd32cf","metadata":{"id":"e0dd32cf"},"outputs":[],"source":["from torch.optim import AdamW\n","from tqdm.auto import tqdm\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(15):\n","    model.train()\n","    for batch in tqdm(train_dataloader):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = None\n","        loss = None\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    total_eval_loss = 0\n","    for batch in eval_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = None\n","        total_eval_loss += None\n","    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n","    print(f\"Average evaluation loss: {avg_eval_loss}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"PkUuGM17z9cD"},"id":"PkUuGM17z9cD","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"pycharm":{"stem_cell":{"cell_type":"raw","source":["# Transfer Learning Transformers with HuggingFace\n","\n","© Data Trainers LLC. GPL v 3.0.\n","\n","Author: Axel Sirota\n"],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":5}