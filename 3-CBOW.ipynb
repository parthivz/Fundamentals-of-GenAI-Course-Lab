{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Continuous Bag of Words (CBOW)\n",
    "\n",
    "Â© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "**Author:** Axel Sirota\n",
    "\n",
    "\n",
    "In this notebook we will train from scratch a CBOW word embedding model based on a famous dataset: The Yelp reviews dataset. This dataset is uploaded into a dropbox and the cell command to download the files is already done for you.\n",
    "\n",
    "Take it easy and pay attention to the model, how easy it is to define it,and the iteration nuances on the dataset generation.\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jbFcxFZhG5K"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade  textblob gensim pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iklSJ4lqUQlT"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "import sys\n",
    "from textblob import TextBlob, Word\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "embedding_dim = 50\n",
    "epochs=100\n",
    "\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  random.seed(42)\n",
    "\n",
    "\n",
    "set_seeds_and_trace()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l13de14sclyD"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvRXU9EMVJMp"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAWXcLEieD4E"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljgSnKkzeM4-"
   },
   "outputs": [],
   "source": [
    "# Create corpus of sentences such that the sentence has more than 3 words\n",
    "corpus = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-AyyCRQ2-7J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point we have a list (any iterable will do) of queries that are longer than 3 words. This is normal to filter random queries. Now we must use the `Tokenizer` object to `fit` on the corpus, in order to convert each wor to an ID, and later convert such corpus of list of words into their identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUlTe1xsgi51"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "# Again, use the LabelEncoder to create the tokenizer and fit it.\n",
    "ids_from_words = None\n",
    "\n",
    "print(f'Before the tokenizer: {corpus[:1]}')\n",
    "\n",
    "#Now use the same \"trained\" tokenizer to convert the corpus from words to IDs with the batch_encode method\n",
    "tokenized_corpus = None\n",
    "\n",
    "print(f'After the tokenizer: {tokenized_corpus[:1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucoEJtOa2-7K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(ids_from_words.vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfR6qIZZhIHd"
   },
   "outputs": [],
   "source": [
    "print(f'First 5 corpus items are {tokenized_corpus[:5]}')\n",
    "print(f'Length of corpus is {len(tokenized_corpus)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B_z5Udki-_s"
   },
   "outputs": [],
   "source": [
    "type(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ids_from_text(text):\n",
    "  return ids_from_words.batch_encode(text)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return ids_from_words.batch_decode(ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pad_sequence_of_tokens(x, maxlen, unk_token='UNK'):\n",
    "  if len(x)<maxlen:\n",
    "    x.extend([unk_token]*(maxlen-len(x)))\n",
    "  return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_Z1eJZrhK7K"
   },
   "outputs": [],
   "source": [
    "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
    "def create_context_target_pairs(texts, context_size):\n",
    "    data = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        for i, word in enumerate(tokens):\n",
    "            start = max(0, i - context_size)\n",
    "            end = min(len(tokens), i + context_size + 1)\n",
    "            context = pad_sequence_of_tokens([tokens[j] for j in range(start, end) if j != i], maxlen=4)\n",
    "            target = ids_from_words.token_to_index[word]\n",
    "            context_indices = [ids_from_words.token_to_index[w] for w in context]\n",
    "            context_indices.append(target)\n",
    "            data.append(torch.Tensor(context_indices))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nfsYbRRS2-7N",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice now in a sample how we construct X and y to predict words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvOclN8T2-7N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = create_context_target_pairs(corpus[:500], 2) # we use 500 words to make the RAM not crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix8s4Knh2-7O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Stack the tensors to create a 2D tensor\n",
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set X, and y\n",
    "\n",
    "X = None\n",
    "y = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpvnqGOI2-7O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now comes the core part, defining the model. Let's add an `Embedding` layer (that will map the word ids into a vector of size 100), a `Lambda` to average the words out in a sentence, and a `Dense layer` to select the best word on the other end. This is classic CBOW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHtu75Kpi6XF"
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = None # Add the embedding layer, which should be the dimensions?\n",
    "        # Linear layer to act as the hidden layer\n",
    "        self.linear1 = None # Make up the hiddden dimension\n",
    "        # Linear layer to predict the center word\n",
    "        self.linear2 = None # Final Linear Layer, how many output neurons should we have?\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds = torch.mean(embeds, dim=1)  # This is to average across words.  key!\n",
    "        out = torch.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g44ICdUcj7ZL"
   },
   "outputs": [],
   "source": [
    "def train_cbow(X, y, model, loss_function, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # Step 1. Recall that torch *accumulates* gradients. Before passing in a new instance,\n",
    "        # you need to zero out the gradients from the old instance\n",
    "        None\n",
    "\n",
    "        # Step 2. Run the forward pass, getting log probabilities over next words\n",
    "        log_probs = None\n",
    "\n",
    "        # Step 3. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = None\n",
    "\n",
    "        # Step 4. Do the backward pass and update the gradient\n",
    "        None\n",
    "        None\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch: {}, Loss: {:.4f}'.format(epoch + 1, total_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTM2wqbzke5n",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "context_size=2\n",
    "embedding_dim=50\n",
    "vocab_size = len(ids_from_words.vocab)\n",
    "model = CBOW(vocab_size, embedding_dim, context_size * 2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_model = train_cbow(X, y, model, loss_function, optimizer, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "embeddings = trained_model.embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "# Now, we need to save these embeddings in a format that gensim can understand\n",
    "# For that, we will use the KeyedVectors instance in gensim\n",
    "\n",
    "# Instantiate the KeyedVectors with the correct size\n",
    "kv = KeyedVectors(vector_size=embeddings.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add the vectors and their corresponding words to the KeyedVectors instance\n",
    "kv.add_vectors(ids_from_words.index_to_token, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kv.most_similar(positive=['gasoline'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kv.most_similar(negative=['apple'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}