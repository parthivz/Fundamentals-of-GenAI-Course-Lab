{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "Â© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "Author: Axel Sirota\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Inspired highly on the tutorial [NMT with Transformers](https://www.tensorflow.org/text/tutorials/transformer) which takes the code from the original Transformer model paper originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017)."
      ],
      "metadata": {
        "id": "gzf37X_XizQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "ovyaAhLpa55P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade  textblob gensim pytorch-nlp swifter\n"
      ],
      "metadata": {
        "id": "C9BTEOu0PerV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da14ae3d-c679-482b-911f-7db826f04a79"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: swifter in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (1.5.3)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (2023.8.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (23.2)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter) (3.17.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.0->swifter) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSaLeBcpqiT5"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "0fpgYwAtNO2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53deabe-f3fc-4611-cfe2-a78e03599556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import sys\n",
        "from textblob import TextBlob, Word\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import swifter\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  torch.manual_seed(0)\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  random.seed(42)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "set_seeds_and_trace()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Layers\n",
        "\n",
        "In this demo we will create, from scratch, with the same tools the original Authors had, the Transformer architecture. Why? To understand how it works, why it works, and exactly what is novel!\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each of the components in these two diagrams will be explained as you progress through the demo.\n"
      ],
      "metadata": {
        "id": "ekM1n0-JdYPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did we have before?"
      ],
      "metadata": {
        "id": "OFbNft2shTPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, we used Cross Attention or self attention, remember? And for sequence data we basically used it like this:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Seq2Seq with attention</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.dropbox.com/s/r6u7ll5nlt96t9f/seq2seq.png?raw=1\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2FoDkeGhWJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where we input attention with the hidden state to create another updated hidden state we could input into the next cell. And this worked well on medium sized sentences, but was hard to train and unstable. Now that we know this, the Transformer basicaly tried to get rid of the RNN by using **only** attention"
      ],
      "metadata": {
        "id": "b3huYRrUjCAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The embedding and positional encoding layer\n",
        "\n",
        "The inputs to both the encoder and decoder use the same embedding and positional encoding logic.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "vwmp1jC0dxVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This comes straight from the paper\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "    depth = depth // 2\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
        "\n",
        "    angle_rates = 1 / (10000 ** depths)\n",
        "    angle_rads = positions * angle_rates\n",
        "\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "    return torch.tensor(pos_encoding, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "UkI09F6zdXnb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.register_buffer('pos_encoding', positional_encoding(2048, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        length = x.size(1)\n",
        "        x = self.embedding(x)\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        x = x + self.pos_encoding[None, :length, :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "gLC8RPIkd--M"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = PositionalEmbedding(5000, 100)"
      ],
      "metadata": {
        "id": "GAnc5AKSeOvn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(np.random.randint(1,5000, size=(3,26)))\n",
        "response = pos(input)\n",
        "response.shape"
      ],
      "metadata": {
        "id": "zlIZmHLMemkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75749e02-4e8c-4f61-eb6a-9047f341fb5b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add and normalize\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>Add and normalize</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "yvlTAaqqhTBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will create a BaseAttention layer that inherits the Add+Norm and then each subclass of attention will implement the correct one"
      ],
      "metadata": {
        "id": "P-ZTlyygheuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(BaseAttention, self).__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In PyTorch, MultiheadAttention expects inputs in the shape (S, N, E)\n",
        "        # S: source sequence length, N: batch size, E: embedding dimension\n",
        "        attn_output, _ = self.mha(x, x, x)\n",
        "        x = x + attn_output\n",
        "        return self.layernorm(x)"
      ],
      "metadata": {
        "id": "OMMyVtc6hLaM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention layer\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "L5Toc-Uukh15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(GlobalSelfAttention, self).__init__(d_model, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transpose to match the input shape requirements of nn.MultiheadAttention\n",
        "        x = x.transpose(0, 1)  # From (N, S, E) to (S, N, E)\n",
        "        return super().forward(x).transpose(0, 1)  # Transpose back to (N, S, E)"
      ],
      "metadata": {
        "id": "RP4L1Mn-iFCl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it!"
      ],
      "metadata": {
        "id": "316Iwonjm6JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = torch.tensor(np.random.randint(1,vocab_size, size=(3,26)))\n",
        "\n",
        "# First we apply the PositionalEmbedding to embed into what the attention layer expects\n",
        "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Then we do the self attention, the n_heads is arbitrary\n",
        "gsa = GlobalSelfAttention(num_heads=5, d_model=embedding_dim)\n",
        "\n",
        "\n",
        "response = gsa(pos(input))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "goX_DO0Sk8UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d101a62-2d4b-4e05-9d21-8b47725635b4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the shape is the same, since MHA concats all 5 heads and the we add everything"
      ],
      "metadata": {
        "id": "n5lGk8ZMle7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The cross attention layer\n",
        "\n",
        "This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the previous demo (and we will copy it).\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The cross attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "J76TA2ZMlrke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(CrossAttention, self).__init__(d_model, num_heads)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = x.transpose(0, 1)  # Transpose to match the shape (S, N, E)\n",
        "        context = context.transpose(0, 1)\n",
        "        attn_output, attn_scores = self.mha(x, context, context, need_weights=True)\n",
        "        self.last_attn_scores = attn_scores\n",
        "        x = x + attn_output\n",
        "        return self.layernorm(x).transpose(0, 1)\n"
      ],
      "metadata": {
        "id": "OxZMLVBRlMPz"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 100\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "\n",
        "pos_es = PositionalEmbedding(vocab_size_es, embedding_dim_es)\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "\n",
        "gsa = GlobalSelfAttention(num_heads=5, d_model=embedding_dim_es)\n",
        "cross = CrossAttention(num_heads=5, d_model=embedding_dim_en)\n",
        "\n",
        "\n",
        "context = gsa(pos_es(input_es)) # Forget about the feed forwards\n",
        "\n",
        "response = cross(pos_en(input_en), context=context) # Forget about masked attention for now, assume it is the identity\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "R9v9MPbvmVJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d695d547-0924-44c8-dc9f-2f88676e90c8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 24, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the shape is (batch_size, words in sentence in output, embedding_dim) , regardless the input sentence had more words or other embedding dim. We are doing a good move forward!"
      ],
      "metadata": {
        "id": "e5bXoAwvoD8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The causal self attention layer (Masked Multi Headed Attention)\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "PuNqXT82oaRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only big difference in the masked multi headedd attention is that we cannot attend to words in the future, so we will use a mask such that the `Nth` word can only see the first `N-1` words and not all the sentence."
      ],
      "metadata": {
        "id": "_-jDIHkJowAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(CausalSelfAttention, self).__init__(d_model, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(0, 1)  # Transpose to match the shape (S, N, E)\n",
        "        attn_mask = torch.triu(torch.ones(x.size(0), x.size(0)), diagonal=1).bool()\n",
        "        attn_output, _ = self.mha(x, x, x, attn_mask=attn_mask)\n",
        "        x = x + attn_output\n",
        "        return self.layernorm(x).transpose(0, 1)"
      ],
      "metadata": {
        "id": "z6N9U3qvn9Ui"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "PgCBG1b-osuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the diagram above how the query can onlly attend the values for the past"
      ],
      "metadata": {
        "id": "AdE_KY2TpEPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_en = 100\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_en = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "csa = CausalSelfAttention(num_heads =5, d_model=embedding_dim_en)\n",
        "\n",
        "response = csa(pos_es(input_en))\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "RaHIa0k4oqD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b775d786-0a70-470c-8cdc-b8bcad1c54dc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 24, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The feed forward network\n",
        "\n",
        "The transformer also includes this point-wise feed-forward network in both the encoder and decoder, we will include them inside the encoder and decoder respectively:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The feed forward network</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "c8_jJCg8pE6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The encoder layer\n",
        "\n",
        "The encoder contains a stack of `N` encoder layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "1Pm7uGPMpqwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dim_feedforward, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, d_model),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        attn_output, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                                        key_padding_mask=src_key_padding_mask)\n",
        "        src = self.norm1(src + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(src)\n",
        "        src = self.norm2(src + self.dropout(ff_output))\n",
        "        return src"
      ],
      "metadata": {
        "id": "3SJl4aJ_pSR0"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = torch.tensor(np.random.randint(1,vocab_size, size=(3,26)))\n",
        "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
        "sample_encoder_layer = EncoderLayer(d_model=embedding_dim, num_heads=5, dim_feedforward=1012)\n",
        "response = sample_encoder_layer(pos(input))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "4NUNWld5uOBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abec2104-bf52-41fa-bec0-cbf4126bc26a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The encoder\n",
        "\n",
        "Notice we need to be able to repeat the past EncoderLayer Nx times, so we need another Layer that is able to do exactly that\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "-MzC1z-vuuxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dim_feedforward, dropout, vocab_size, max_seq_length=5000):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        src = self.norm(src)\n",
        "        return src\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "0TlZP_vVulm0"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = torch.tensor(np.random.randint(1,vocab_size, size=(3,26)))\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=embedding_dim,\n",
        "                         num_heads=5,\n",
        "                         dim_feedforward=512,\n",
        "                         dropout=0.1,\n",
        "                         vocab_size=vocab_size)\n",
        "response = sample_encoder(input)\n",
        "response.shape"
      ],
      "metadata": {
        "id": "Yf9wGOSMwaEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92146c9f-ae1e-4888-836f-cb03182dc4eb"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got our Encoder!! Yahoo!!"
      ],
      "metadata": {
        "id": "uUU9CG_MwtJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The decoder layer\n",
        "\n",
        "Same as before we need a Decoder layer that uses the Attention layers and then another layer to permit having Nx layers of decoding\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The decoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "UR4MuroWw4p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dim_feedforward, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, d_model),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        tgt_reshaped = tgt.transpose(0, 1)\n",
        "        memory_reshaped = memory.transpose(0, 1)\n",
        "\n",
        "        # Self attention\n",
        "        tgt2 = self.self_attn(tgt_reshaped, tgt_reshaped, tgt_reshaped, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = self.norm1(tgt_reshaped + tgt2)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        # Reshape back to (batch_size, seq_length, d_model)\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "\n",
        "        # Cross attention with encoder's output\n",
        "        tgt_reshaped = tgt.transpose(0, 1)  # Reshape again for cross-attention\n",
        "        tgt2 = self.multihead_attn(tgt_reshaped, memory_reshaped, memory_reshaped, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = self.norm2(tgt_reshaped + tgt2)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        # Reshape back to (batch_size, seq_length, d_model)\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "\n",
        "        # Feed forward\n",
        "        tgt2 = self.feed_forward(tgt)\n",
        "        tgt = self.norm3(tgt + tgt2)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        return tgt"
      ],
      "metadata": {
        "id": "_v5voBr2wywC"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 100\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "\n",
        "encoder =  Encoder(num_layers=4,\n",
        "                         d_model=embedding_dim_es,\n",
        "                         num_heads=5,\n",
        "                         dim_feedforward=512,\n",
        "                         dropout=0.1,\n",
        "                         vocab_size=vocab_size)\n",
        "\n",
        "context = encoder(input_es)\n",
        "\n",
        "print(pos_en(input_en).shape, context.shape)\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model=embedding_dim_en, num_heads=5, dim_feedforward=218, dropout=0.2)\n",
        "\n",
        "response = decoder_layer(pos_en(input_en), memory=context)\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "aP3yjQeuy2n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b18a343-6334-4948-eaa6-9f2e672baafe"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 24, 100]) torch.Size([3, 26, 100])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 24, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder\n",
        "\n",
        "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "pog5S-gQz9nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dim_feedforward, dropout, vocab_size, max_seq_length=5000):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                        tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        tgt = self.norm(tgt)\n",
        "        return tgt"
      ],
      "metadata": {
        "id": "dEZ2-NRbz1jz"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 100\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "\n",
        "encoder =  Encoder(num_layers=4,\n",
        "                         d_model=embedding_dim_es,\n",
        "                         num_heads=5,\n",
        "                         dim_feedforward=512,\n",
        "                         dropout=0.1,\n",
        "                         vocab_size=vocab_size)\n",
        "\n",
        "context = encoder(input_es)\n",
        "\n",
        "decoder = Decoder(num_layers=4, d_model=embedding_dim_en, num_heads=5, dim_feedforward=124, dropout=0.1, vocab_size=vocab_size_en)\n",
        "\n",
        "response = decoder(input_en, memory=context)\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "Mp4zkTw22-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f908d57-78a2-4f77-a392-ab8748811af4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 24, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Model\n",
        "\n",
        "You now have `Encoder` and `Decoder`. To complete the `Transformer` model, you need to put them together and add a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities.\n",
        "\n",
        "The output of the decoder is the input to this final linear layer.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The transformer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3r2JLfz04Hsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_vocab_size, tgt_vocab_size, d_model):\n",
        "        super(Translator, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask):\n",
        "        memory = self.encoder(src, src_mask, src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask, memory_mask=None,\n",
        "                              tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        output = self.output_layer(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "VrXSLwrF36u_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size_es = 5000\n",
        "vocab_size_en = 6000\n",
        "\n",
        "num_layers = 4\n",
        "dim_feedforward = 512\n",
        "num_heads = 5\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
        "input_en = torch.tensor(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
        "src_mask = tgt_mask = torch.ones((3, 3)).to(torch.bool)\n",
        "\n",
        "encoder =  Encoder(num_layers=num_layers,\n",
        "                         d_model=embedding_dim,\n",
        "                         num_heads=num_heads,\n",
        "                         dim_feedforward=dim_feedforward,\n",
        "                         dropout=dropout_rate,\n",
        "                         vocab_size=vocab_size_es)\n",
        "\n",
        "context = encoder(input_es)\n",
        "\n",
        "decoder = Decoder(num_layers=num_layers, d_model=embedding_dim, num_heads=num_heads, dim_feedforward=dim_feedforward, dropout=dropout_rate, vocab_size=vocab_size_en)\n",
        "\n",
        "\n",
        "\n",
        "translator = Translator(encoder, decoder, vocab_size_es, vocab_size_en, d_model=embedding_dim)\n",
        "\n",
        "output = translator(input_es, input_en, src_mask, None, None, None, None)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "7CIxbfVx9IBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e7a63e-a576-4a7b-c72f-2f018fcd0fe1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 24, 6000])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator"
      ],
      "metadata": {
        "id": "Gy5B7d_E-NDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62f2617-9086-4ba6-979b-2d2f6ebb2dc1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Translator(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5000, 100)\n",
              "    (pos_encoder): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x EncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "        )\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=100, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Linear(in_features=512, out_features=100, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(6000, 100)\n",
              "    (pos_encoder): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x DecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "        )\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=100, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Linear(in_features=512, out_features=100, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=100, out_features=6000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqnWfvyM7XDJ"
      },
      "execution_count": 80,
      "outputs": []
    }
  ]
}